# -*- coding: utf-8 -*-
"""XAI_Subjectivity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jUQld-Wwu6TgOnnQZRBFpm1MdgzhuLLT
"""

!pip install nltk
!pip install spacy

import nltk
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

!pip install pandas
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,f1_score, precision_score, recall_score

cd /content/drive/MyDrive/My_Experiment/Decision_Tree

!pwd

train_df = pd.read_csv('Hindi_Hasoc2019.tsv',delimiter="\t")
train_df= train_df.dropna(how='all')
train_df = train_df.rename(columns={'text': 'text','task_1': 'labels', })
train = train_df[['text', 'labels']]

train["tweetid"] = train.index
#train.head(10)

import re

def remove_urls(text):
    # Define the regex pattern for URLs starting with http or https
    url_pattern = re.compile(r'http[s]?://\S+')
    # Substitute the URLs with an empty string
    cleaned_text = url_pattern.sub('', text)
    return cleaned_text.strip()

import re
def remove_emojis_and_smiles(text):
    # Regex pattern to match emojis
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F700-\U0001F77F"  # alchemical symbols
                           u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                           u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                           u"\U0001F900-\U0001FAFF"  # Supplemental Symbols and Pictographs
                           u"\U0001FB00-\U0001FBFF"  # Symbols and Pictographs Extended-A
                           u"\U0001FC00-\U0001FFFF"  # reserved
                           u"\U00002702-\U000027B0"  # Dingbats
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

    # Remove emojis
    text = emoji_pattern.sub(r'', text)

    # Regex pattern to match common smiley faces
    smiley_pattern = re.compile(r'[:;]-?[)D(]|[)D]-?[:;]')

    # Remove smiley faces
    text = smiley_pattern.sub(r'', text)

    return text

import html
import re

# Regular expression to replace multiple spaces with a single space
re1 = re.compile(r' +')

# Regular expression to remove special symbols except for the ones you want to keep
special_symbols_re = re.compile(r'[@#&<>^*(){}[\]|\\;:"`~!$%^+=?,./]')
from spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI

def textFixup(aText):
    # Replace specific HTML entities and other patterns
    aText = aText.replace('#39;', "'").replace('amp;', '&').replace('#146;', "'").replace(
        'nbsp;', ' ').replace('#36;', '$').replace('\\n', "\n").replace('quot;', "'").replace(
        '<br />', "\n").replace('\\"', '"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(
        ' @-@ ', '-').replace('\\', ' \\ ').replace('â€™', "'")

    # Remove special symbols
    aText = special_symbols_re.sub(' ', aText)
    aText = [word for word in aText.split() if word not in set(STOP_WORDS_HI) ]
    aText=' '.join(aText)
    # Replace multiple spaces with a single space and unescape HTML entities
    return re1.sub(' ', html.unescape(aText))

import html
def preprocess_aTweet(tweet):
    tweet = tweet.lower()
    tweet = textFixup(tweet)
    return tweet

X_train, X_test, y_train, y_test = train_test_split(train[["text"]],train[["labels"]], test_size=0.25, random_state=0)
X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)

X_train_processed = X_train['text'].apply(lambda x: remove_urls(x))
X_test_processed = X_test['text'].apply(lambda x: remove_urls(x))

X_train_processed = X_train['text'].apply(lambda x: preprocess_aTweet(x))
X_test_processed = X_test['text'].apply(lambda x: preprocess_aTweet(x))

"""Setting display for non-English text"""

import matplotlib.pyplot as plt
from matplotlib import font_manager as fm

# Paths to the font files
hindi_font_path = 'Lohit-Devanagari.ttf'
english_font_path = 'Arial-Regular.ttf'

# Add the fonts to Matplotlib
fm.fontManager.addfont(hindi_font_path)
fm.fontManager.addfont(english_font_path)

# Load the fonts using FontProperties
hindi_font_prop = fm.FontProperties(fname=hindi_font_path)
english_font_prop = fm.FontProperties(fname=english_font_path)

# Set the default font family to use both fonts
plt.rcParams['font.family'] = [english_font_prop.get_name(), hindi_font_prop.get_name()]

# Print the names of the fonts to ensure they're loaded correctly
print(f"Loaded English font: {english_font_prop.get_name()}")
print(f"Loaded Hindi font: {hindi_font_prop.get_name()}")

"""BERTopic Topic Modelling"""

tweets=train["text"]

def remove_https_urls(text):
    # Define the regex pattern for HTTP and HTTPS URLs
    url_pattern = r'http[s]?://[^\s]+'

    # Use re.sub() to replace the URLs with an empty string
    cleaned_text = re.sub(url_pattern, '', text)

    return cleaned_text

tweets = [remove_https_urls(tweet) for tweet in tweets]

preprocessed_tweets = [textFixup(tweet) for tweet in tweets]

pip install BERTopic

from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN

from bertopic import BERTopic

embedding_model = SentenceTransformer("sentence-transformers/LaBSE")



from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance

main_representation_model = KeyBERTInspired()

representation_model = {
   "Main": main_representation_model
}


vectorizer_model = TfidfVectorizer(tokenizer=word_tokenize,token_pattern=None,ngram_range=(1,1))


umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.1, metric='cosine',random_state=42)


hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=1)

topic_model = BERTopic(nr_topics=15,embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model,verbose=True,language="multilingual",vectorizer_model = vectorizer_model,
                      representation_model = representation_model)


topics, probabilities = topic_model.fit_transform(preprocessed_tweets)

topic_model.visualize_barchart(top_n_topics = 16, n_words = 10)

len(topics)

tweets_dict={}
tweets_dict["tweetid"]=[]
tweets_dict["text"]=[]
tweets_dict["topic"]=[]
tweets_dict["labels"]=[]
for i, (tweet, topic) in enumerate(zip(tweets, topics)):
    tweets_dict["tweetid"].append(i)
    tweets_dict["text"].append(tweet)
    tweets_dict["topic"].append(topic)
    tweet_label=train.loc[train['tweetid'] == i, 'labels'].iloc[0]
    tweets_dict["labels"].append(tweet_label)

tweets_df=pd.DataFrame.from_dict(tweets_dict)
tweets_df.head(10)

topic_model.get_topic_info(0)

freq=topic_model.get_topic_info()
len(freq)

topic_model.get_topic_info(-1)

l=len(freq)
topic_info_list=[]
for i in range(l):
  topic_info=topic_model.get_topic_info(i)
  topic_info_list.append(topic_info)

topics_info_df=pd.concat(topic_info_list,ignore_index=True)

topics_info_df

import pandas as pd
df = pd.DataFrame({'Tweet': tweets, 'Interesting topics': topics})
df

freq=topic_model.get_topic_info()
len(freq)

l=len(freq)
topics_info_dfs={}
for i in range(l):
  topics_info_dfs[f'topic_{i}']=pd.DataFrame(topic_model.get_topic_info(i))

"""LogisticRegression"""

from sklearn.linear_model import LogisticRegression

# Load the DataFrame
outliers = tweets_df.loc[tweets_df['topic'] == -1]
df_topic_0 = tweets_df.loc[tweets_df['topic'] == 0]
df_topic_1 = tweets_df.loc[tweets_df['topic'] == 1]
df_topic_2 = tweets_df.loc[tweets_df['topic'] == 2]
df_topic_3 = tweets_df.loc[tweets_df['topic'] == 3]
df_topic_4 = tweets_df.loc[tweets_df['topic'] == 4]

print(len(outliers))

!pip install openpyxl
#df = pd.read_excel("/content/drive/MyDrive/My_Experiment/Decision_Tree/InCorrect_Labels2.xlsx",engine='openpyxl')
df = pd.read_excel("InCorrect_Labels2.xlsx",engine='openpyxl')
df=df.dropna()

#df = pd.read_excel("InCorrect_Labels.xlsx")
#df=df.dropna()
df.head(6)

df['Tweet ID'] = df['Tweet ID'].astype(int)

df.loc[:, "Tweet ID"] = df["Tweet ID"].apply(lambda x: x - 1)
ids_to_check = df["Tweet ID"].values.tolist()
print(ids_to_check)

print(len(ids_to_check))

filtered_df_0 = df_topic_0[df_topic_0['tweetid'].isin(ids_to_check)]
print(filtered_df_0.shape)
filtered_df_0["labels"].value_counts()
#df['newlabels'] = ~filtered_df_0['labels']
filtered_df_0['newlabels'] = np.where(filtered_df_0['labels']== 'HOF', 'NOT', 'HOF')

X_train, X_test, y_train, y_test = train_test_split(filtered_df_0["text"], filtered_df_0[['labels']])



tfidf_vectorizer1 = TfidfVectorizer()

X_train_tf = tfidf_vectorizer1.fit_transform(X_train)
# transform the test set with vectoriser
X_test_tf = tfidf_vectorizer1.transform(X_test)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)
y_train_encoded = label_encoder.fit_transform(y_train)

# create logistic regression model
logreg = LogisticRegression(verbose=1, random_state=0, penalty='l2', solver='newton-cg')
# train model on  vectorised training data
model = logreg.fit(X_train_tf, y_train_encoded)
# evaluate model performance on the test set
pred = model.predict(X_test_tf)
metrics.f1_score(y_test_encoded, pred, average='weighted')

filtered_df_1 = df_topic_1[df_topic_1['tweetid'].isin(ids_to_check)]
filtered_df_1.shape

outlier_df = outliers[outliers['tweetid'].isin(ids_to_check)]
outlier_df.shape

filtered_df_2 = df_topic_2[df_topic_2['tweetid'].isin(ids_to_check)]
filtered_df_2.shape

filtered_df_3 = df_topic_3[df_topic_3['tweetid'].isin(ids_to_check)]
filtered_df_3.shape

filtered_df_4 = df_topic_4[df_topic_4['tweetid'].isin(ids_to_check)]
filtered_df_4.shape

import numpy as np
filtered_notdf_0=df_topic_0[np.logical_not(df_topic_0["tweetid"].isin(ids_to_check))]
print(filtered_notdf_0.shape)
filtered_notdf_0["labels"].value_counts()

X_train, X_test, y_train, y_test = train_test_split(filtered_notdf_0["text"], filtered_notdf_0[['labels']])


tfidf_vectorizer1 = TfidfVectorizer()

X_train_tf = tfidf_vectorizer1.fit_transform(X_train)
# transform the test set with vectoriser
X_test_tf = tfidf_vectorizer1.transform(X_test)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)
y_train_encoded = label_encoder.fit_transform(y_train)

# create logistic regression model
logreg = LogisticRegression(verbose=1, random_state=0, penalty='l2', solver='newton-cg')
# train model on  vectorised training data
model = logreg.fit(X_train_tf, y_train_encoded)
# evaluate model performance on the test set
pred = model.predict(X_test_tf)
metrics.f1_score(y_test_encoded, pred, average='weighted')

incorrectlabels_df = filtered_df_0
correctlabels_df=filtered_notdf_0.sample(n=120, random_state=1)
merged_df=pd.concat([incorrectlabels_df, correctlabels_df], axis=0)

#new_df=new_df.dropna()
merged_df.shape

X_train, X_test, y_train, y_test = train_test_split(merged_df["text"], merged_df[['labels']])


tfidf_vectorizer1 = TfidfVectorizer()

X_train_tf = tfidf_vectorizer1.fit_transform(X_train)
# transform the test set with vectoriser
X_test_tf = tfidf_vectorizer1.transform(X_test)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)
y_train_encoded = label_encoder.fit_transform(y_train)

# create logistic regression model
logreg = LogisticRegression(verbose=1, random_state=0, penalty='l2', solver='newton-cg')
# train model on  vectorised training data
model = logreg.fit(X_train_tf, y_train_encoded)
# evaluate model performance on the test set
pred = model.predict(X_test_tf)
metrics.f1_score(y_test_encoded, pred, average='weighted')

incorrectlabels_df.head()

import csv
to_write = [['tweetid', 'tweet', 'label','roman','english','labelchange']]
incorrect_labels_tweetids_0=incorrectlabels_df["tweetid"].values.tolist()
correct_labels_tweetdict=correctlabels_df.to_dict("records")

dictionary_incorrect_labels=df.to_dict("records")
for item in dictionary_incorrect_labels:
    if (item["Tweet ID"]) in incorrect_labels_tweetids_0:
        to_write.append([item["Tweet ID"],item["Tweet"],item["Label"],item["Roman Hindi"],item["English"],True])
for item in correct_labels_tweetdict:
    to_write.append([item["tweetid"],item["text"],item["labels"],"n/a","n/a",False])



file_path = "metadata_HASOC2019_annotations.csv"

with open(file_path, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(to_write)

metadatafeatures_hasoc = pd.read_csv('metadata_HASOC2019_annotations_final.csv')
metadatafeatures_hasoc=metadatafeatures_hasoc.drop('Unnamed: 10', axis=1)
metadatafeatures_hasoc=metadatafeatures_hasoc.drop('Unnamed: 11', axis=1)
metadatafeatures_hasoc=metadatafeatures_hasoc.dropna()
metadatafeatures_hasoc['labelchange'] = metadatafeatures_hasoc['labelchange'].map({True: 1, False: 0})

metadatafeatures_hasoc.head()

X_features=metadatafeatures_hasoc[["entity", "politicalentity","swearwords","targetgroup"]]
target=metadatafeatures_hasoc["labelchange"]
print(target)
x_train, x_test, y_train, y_test = train_test_split(X_features.values, target, stratify=target, test_size=0.3, random_state=8)
#print(x_train)

!pip install pytorch-tabnet

import pytorch_tabnet
from pytorch_tabnet.tab_model import TabNetClassifier
import torch

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, accuracy_score

clf1_nopreproc = TabNetClassifier(optimizer_fn=torch.optim.Adam,
                       optimizer_params=dict(lr=2e-2),
                       scheduler_params={"step_size":10, # how to use learning rate scheduler
                                         "gamma":0.9},
                       scheduler_fn=torch.optim.lr_scheduler.StepLR,
                       mask_type='entmax' # "sparsemax"
                      )

# fit the model
clf1_nopreproc.fit(
    x_train,y_train,
    eval_set=[(x_train, y_train), (x_test, y_test)],
    eval_name=['train', 'valid'],
    eval_metric=['balanced_accuracy'],
    max_epochs=1000 , patience=50,
    batch_size=32, virtual_batch_size=32,
    num_workers=0,
    weights=1,
    drop_last=False
)

# find and plot feature importance
y_pred = clf1_nopreproc.predict(x_test)
clf1_nopreproc.feature_importances_
feat_importances = pd.Series(clf1_nopreproc.feature_importances_, index=X_features.columns)
feat_importances.nlargest(20).plot(kind='barh')


# determine best accuracy for test set
preds = clf1_nopreproc.predict(x_test)
test_acc = accuracy_score(preds, y_test)

# determine best accuracy for validation set
preds_valid = clf1_nopreproc.predict(x_test)
valid_acc = accuracy_score(preds_valid, y_test)

print(f"BEST ACCURACY SCORE ON VALIDATION SET : {valid_acc}")
print(f"BEST ACCURACY SCORE ON TEST SET : {test_acc}")